{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/caption-generator/blob/main/train_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPqWBnBr3qyC",
        "outputId": "62da8d48-9696-47a4-ae3c-0db9425e8a0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    os.system('git clone https://github.com/tikendraw/caption-generator.git -q')\n",
        "    os.chdir('caption-generator')\n",
        "\n",
        "\n",
        "if not os.path.exists('funcyou'):\n",
        "\tos.system('git clone https://github.com/tikendraw/funcyou -q')\n",
        "\n",
        "os.system('pip install funcyou/. -q')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/t/miniconda3/envs/tf_new/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "funcyou                      1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep funcyou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5cASyj73_nu",
        "outputId": "4d3979e6-0d5b-40de-c472-032da8229a28"
      },
      "outputs": [],
      "source": [
        "# ! pip install funcyou/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPhVMf_b419N",
        "outputId": "934a5a3f-3ac0-413e-87af-5ea1e2b072be"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, math\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import datetime\n",
        "import sys\n",
        "from functools import cache\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import regex as re\n",
        "import yaml\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocessing\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization, Embedding, LSTM, GRU, Bidirectional, TimeDistributed, Dense, Attention, MultiHeadAttention, Flatten, Dropout,\n",
        "    Concatenate, Activation, GlobalAveragePooling2D\n",
        "    )\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Input, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import array_to_img, img_to_array\n",
        "import string\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, TensorBoard\n",
        "from model import LearningRateDecayCallback, get_model, masked_acc, masked_loss\n",
        "from preprocessing import preprocess_text, embedding_matrix_creater, mapper, clean_words, clean_df\n",
        "from utils import create_model_checkpoint\n",
        "\n",
        "from config import config\n",
        "\n",
        "from get_data import download_dataset\n",
        "from funcyou.dataset import download_kaggle_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the path to your config file\n",
        "config_file_path = './config.yaml'\n",
        "\n",
        "# Read the config file and load its content into a Python object\n",
        "with open(config_file_path, 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "\n",
        "\n",
        "SEED_VALUE                              = config['seed_value']\n",
        "RAW_CAPTION_FILE                        = config['raw_caption_file']\n",
        "CAPTION_FILE                            = config['caption_file']\n",
        "IMAGE_DIR                               = config['image_dir']\n",
        "IMG_SIZE                                = config['img_size']\n",
        "CHANNELS                                = config['channels']\n",
        "IMG_SHAPE                               = config['img_shape']\n",
        "MAX_LEN                                 = config['max_len']\n",
        "BATCH_SIZE                              = config['batch_size']\n",
        "EPOCHS                                  = config['epochs']\n",
        "LEARNING_RATE                           = config['learning_rate']\n",
        "UNITS                                   = config['units']\n",
        "TEST_SIZE                               = config['test_size']\n",
        "VALIDATION_SIZE                         = config['validation_size']\n",
        "EMBEDDING_DIMENSION                     = config['embedding_dimension']\n",
        "GLOVE_PATH                              = config['glove_path']\n",
        "D_MODEL                                 = config['d_model']\n",
        "NUM_HEADS                               = config['num_heads']    \n",
        "PATCH_SIZE                              = config['patch_size']    \n",
        "TRANSFORMER_LAYERS                      = config['transformer_layers']            \n",
        "NUM_LAYERS                              = config['num_layers']\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KdH2ocgf5DVi"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A3qtMoVA_lf_"
      },
      "outputs": [],
      "source": [
        "pathh = '/content/kaggle.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wi7t3Bs5KMz",
        "outputId": "90375520-20c5-49b4-84f0-818edb5fd8f8"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "    download_dataset(pathh)\n",
        "    # ! rm -rf flickr30k_images\n",
        "\n",
        "#GLOVE embedding\n",
        "glove_api_command = 'kaggle datasets download -d watts2/glove6b50dtxt'\n",
        "glove_url = 'https://www.kaggle.com/datasets/watts2/glove6b50dtxt'\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "\n",
        "    download_kaggle_dataset(glove_api_command)\n",
        "    os.makedirs('embedding', exist_ok = True)\n",
        "    shutil.move('glove6b50dtxt.zip', 'embedding/glove.6B.50d.zip',)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S2RsNzwVEoIU"
      },
      "source": [
        "# Reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "IxFlSCnqDIxq",
        "outputId": "3ea1d4f8-17b1-4f1e-863a-e4591b005971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 158914 entries, 0 to 158913\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count   Dtype \n",
            "---  ------          --------------   ----- \n",
            " 0   Unnamed: 0      158914 non-null  int64 \n",
            " 1   image_name      158914 non-null  object\n",
            " 2   comment_number  158914 non-null  int64 \n",
            " 3   comment         158914 non-null  object\n",
            " 4   word_length     158914 non-null  int64 \n",
            " 5   image_path      158914 non-null  object\n",
            "dtypes: int64(3), object(3)\n",
            "memory usage: 7.3+ MB\n",
            "None\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>image_name</th>\n",
              "      <th>comment_number</th>\n",
              "      <th>comment</th>\n",
              "      <th>word_length</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>startseq Two young guys with shaggy hair look ...</td>\n",
              "      <td>17</td>\n",
              "      <td>input/flickr30k/images/1000092795.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>startseq Two young White males are outside nea...</td>\n",
              "      <td>11</td>\n",
              "      <td>input/flickr30k/images/1000092795.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>startseq Two men in green shirts are standing ...</td>\n",
              "      <td>11</td>\n",
              "      <td>input/flickr30k/images/1000092795.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>startseq A man in a blue shirt standing in a g...</td>\n",
              "      <td>11</td>\n",
              "      <td>input/flickr30k/images/1000092795.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>startseq Two friends enjoy time together endseq</td>\n",
              "      <td>7</td>\n",
              "      <td>input/flickr30k/images/1000092795.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0      image_name  comment_number   \n",
              "0           0  1000092795.jpg               0  \\\n",
              "1           1  1000092795.jpg               1   \n",
              "2           2  1000092795.jpg               2   \n",
              "3           3  1000092795.jpg               3   \n",
              "4           4  1000092795.jpg               4   \n",
              "\n",
              "                                             comment  word_length   \n",
              "0  startseq Two young guys with shaggy hair look ...           17  \\\n",
              "1  startseq Two young White males are outside nea...           11   \n",
              "2  startseq Two men in green shirts are standing ...           11   \n",
              "3  startseq A man in a blue shirt standing in a g...           11   \n",
              "4    startseq Two friends enjoy time together endseq            7   \n",
              "\n",
              "                              image_path  \n",
              "0  input/flickr30k/images/1000092795.jpg  \n",
              "1  input/flickr30k/images/1000092795.jpg  \n",
              "2  input/flickr30k/images/1000092795.jpg  \n",
              "3  input/flickr30k/images/1000092795.jpg  \n",
              "4  input/flickr30k/images/1000092795.jpg  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(CAPTION_FILE)\n",
        "\n",
        "print(df.info())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVORBQxSE2tn",
        "outputId": "fe9ba702-3994-4f43-8c6d-8aa68a598133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31783, 6)\n"
          ]
        }
      ],
      "source": [
        "df = df[df.comment_number == 1]\n",
        "print(df.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aTMyFjg3C4mo"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tg2RRXBWC3pQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#tokenizer\n",
        "tokenizer = TextVectorization(standardize=preprocess_text)\n",
        "tokenizer.adapt(df['comment'])\n",
        "\n",
        "\n",
        "word_to_id = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "id_to_word = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mapper(x, y, tokenizer):\n",
        "    x = load_images_now(x)\n",
        "    y = tokenizer(y)\n",
        "\n",
        "    y_in = y[:-1]\n",
        "    y_in =  tf.pad(y_in, [[0, MAX_LEN - tf.shape(y_in)[0]]] , constant_values=0)\n",
        "\n",
        "    y_out = y[1:]\n",
        "    y_out =  tf.pad(y_out, [[0, MAX_LEN - tf.shape(y_out)[0]]], constant_values=0)\n",
        "\n",
        "    return (x, y_in), y_out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def load_images_now(x):\n",
        "    image_data = tf.io.read_file(x)\n",
        "    image_features = tf.image.decode_jpeg(image_data, channels=CHANNELS)\n",
        "    image_features = tf.image.resize_with_pad(\n",
        "        image_features, target_height=IMG_SIZE, target_width=IMG_SIZE)\n",
        "\n",
        "    return image_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1Hw6gq4hIR",
        "outputId": "9b8ed9ff-5e58-40fc-e8d4-4c9a23f2ebf7"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m word_dict \u001b[39m=\u001b[39m {word: i \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokenizer\u001b[39m.\u001b[39mget_vocabulary())}\n\u001b[1;32m      4\u001b[0m \u001b[39m# Creating embedding matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m embedding_matrix \u001b[39m=\u001b[39m embedding_matrix_creater(EMBEDDING_DIMENSION, word_index\u001b[39m=\u001b[39;49mword_dict)\n\u001b[1;32m      7\u001b[0m \u001b[39m# # Saving embedding_matrix for further use\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# np.save(\"./embedding/embedding_matrix.npy\", embedding_matrix, allow_pickle=True)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# # compressing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[39m# load image model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m resnet \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mapplications\u001b[39m.\u001b[39mResNet50V2(\n\u001b[1;32m     16\u001b[0m     include_top\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     weights\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     input_tensor\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39mIMG_SHAPE))\n",
            "File \u001b[0;32m~/cproject/caption-generator/preprocessing.py:147\u001b[0m, in \u001b[0;36membedding_matrix_creater\u001b[0;34m(EMBEDDING_DIMS, word_index)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membedding_matrix_creater\u001b[39m(EMBEDDING_DIMS, word_index):\n\u001b[0;32m--> 147\u001b[0m     embeddings_index \u001b[39m=\u001b[39m glove_embedding(glove_path)\n\u001b[1;32m    148\u001b[0m     embedding_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(word_index), EMBEDDING_DIMS))\n\u001b[1;32m    149\u001b[0m     \u001b[39mfor\u001b[39;00m word, i \u001b[39min\u001b[39;00m word_index\u001b[39m.\u001b[39mitems():\n",
            "File \u001b[0;32m~/cproject/caption-generator/preprocessing.py:130\u001b[0m, in \u001b[0;36mglove_embedding\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mglove_embedding\u001b[39m(path:Path) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[39mwith\u001b[39;00m ZipFile(path) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 130\u001b[0m         f\u001b[39m.\u001b[39;49mextractall(\u001b[39m\"\u001b[39;49m\u001b[39m./embedding/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    133\u001b[0m     embeddings_index \u001b[39m=\u001b[39m {}\n\u001b[1;32m    134\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.6B.50d.txt\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
            "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/zipfile.py:1642\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[1;32m   1641\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[0;32m-> 1642\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
            "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/zipfile.py:1697\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[1;32m   1695\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[1;32m   1696\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[0;32m-> 1697\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfileobj(source, target)\n\u001b[1;32m   1699\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
            "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/shutil.py:208\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m fdst_write(buf)\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ],
      "source": [
        "\n",
        "# creating embedding matrix\n",
        "word_dict = {word: i for i, word in enumerate(tokenizer.get_vocabulary())}\n",
        "\n",
        "# Creating embedding matrix\n",
        "embedding_matrix = embedding_matrix_creater(EMBEDDING_DIMENSION, word_index=word_dict)\n",
        "\n",
        "# # Saving embedding_matrix for further use\n",
        "# np.save(\"./embedding/embedding_matrix.npy\", embedding_matrix, allow_pickle=True)\n",
        "# # compressing\n",
        "# ZipFile(\"embedding_matrix.zip\", mode=\"w\").write(\n",
        "#     \"./embedding/embedding_matrix.npy\")\n",
        "\n",
        "\n",
        "# load image model\n",
        "resnet = tf.keras.applications.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=tf.keras.layers.Input(shape=IMG_SHAPE))\n",
        "\n",
        "resnet.trainable = False\n",
        "\n",
        "\n",
        "# Creating dataset\n",
        "TEST_SIZE = config.TEST_SIZE\n",
        "VAL_SIZE =  config.VAL_SIZE\n",
        "\n",
        "train, val = train_test_split(\n",
        "    df[['image_path', 'comment']],  test_size=VAL_SIZE, random_state=11)\n",
        "train, test = train_test_split(\n",
        "    train[['image_path', 'comment']],  test_size=TEST_SIZE, random_state=11)\n",
        "\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train.image_path, train.comment))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test.image_path, test.comment))\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val.image_path, val.comment))\n",
        "\n",
        "\n",
        "train_data = train_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_data =   test_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_data =     val_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# resnet_output_flattened_shape = 8*8*2048\n",
        "\n",
        "print(\"Number of training samples: %d\" %\n",
        "      tf.data.experimental.cardinality(train_data))\n",
        "print(\"Number of validation samples: %d\" %\n",
        "      tf.data.experimental.cardinality(val_data))\n",
        "print(\"Number of test samples: %d\" %\n",
        "      tf.data.experimental.cardinality(test_data))\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocabulary_size()\n",
        "print(\"Vocabulary size: %d\" % VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bhgJTkV93Lbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image_input:  (None, 2048)\n",
            "enc dropout:  (None, 2048)\n",
            "Dense:  (None, 800)\n",
            "reshape:  (None, 50, 16)\n",
            "txt_input:  (None, 50)\n",
            "text_embedding:  (None, 50, 50)\n",
            "encoder output:  (None, 50, 16)\n",
            "enc dropout:  (None, 50, 16)\n",
            "decoder output:  (None, 50, 16)\n",
            "decoder dropout:  (None, 50, 16)\n",
            "attention:  (None, 50, 16) (None, 50, 16)\n",
            "concat:  (None, 50, 64)\n",
            "concat dropout:  (None, 50, 64)\n",
            "Dense1:  (None, 50, 800)\n",
            "Dense2:  (None, 50, 800)\n",
            "Dense3_final:  (None, 50, 7653)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text_input (InputLayer)        [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 50)       382650      ['text_input[0][0]']             \n",
            "                                                                                                  \n",
            " image_input (InputLayer)       [(None, 2048)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 50, 16),     4288        ['embedding[0][0]']              \n",
            "                                 (None, 16),                                                      \n",
            "                                 (None, 16)]                                                      \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 2048)         0           ['image_input[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 50, 16)       0           ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 800)          1639200     ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 50, 16)       2112        ['dropout_1[0][0]',              \n",
            "                                                                  'lstm[0][1]',                   \n",
            "                                                                  'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " tf.reshape (TFOpLambda)        (None, 50, 16)       0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 50, 16)       0           ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, 50, 16)       0           ['tf.reshape[0][0]',             \n",
            "                                                                  'dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " attention_1 (Attention)        (None, 50, 16)       0           ['dropout_2[0][0]',              \n",
            "                                                                  'tf.reshape[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 50, 64)       0           ['tf.reshape[0][0]',             \n",
            "                                                                  'dropout_2[0][0]',              \n",
            "                                                                  'attention[0][0]',              \n",
            "                                                                  'attention_1[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 50, 64)       0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50, 800)      52000       ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 50, 800)      640800      ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 50, 7653)     6130053     ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,851,103\n",
            "Trainable params: 8,468,453\n",
            "Non-trainable params: 382,650\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = get_model(embedding_matrix, VOCAB_SIZE)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSSztQsfRuzJ",
        "outputId": "5eef9e11-8775-4489-b2d0-18af0a3d03b0"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer \"attention\" (type Attention).\n\nDimensions must be equal, but are 50 and 16 for '{{node attention/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=true](Placeholder, Placeholder_1)' with input shapes: [?,16,50], [?,50,16].\n\nCall arguments received by layer \"attention\" (type Attention):\n  • inputs=['tf.Tensor(shape=(None, 16, 50), dtype=float32)', 'tf.Tensor(shape=(None, 50, 16), dtype=float32)']\n  • mask=['tf.Tensor(shape=(None, 16), dtype=bool)', 'tf.Tensor(shape=(None, 50), dtype=bool)']\n  • training=None\n  • return_attention_scores=False\n  • use_causal_mask=False",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m get_model(embedding_matrix, VOCAB_SIZE)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mLEARNING_RATE),\n\u001b[1;32m      5\u001b[0m               loss\u001b[39m=\u001b[39mmasked_loss,\n\u001b[1;32m      6\u001b[0m               metrics\u001b[39m=\u001b[39m[masked_acc, masked_loss])\n",
            "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(embedding_matrix, VOCAB_SIZE)\u001b[0m\n\u001b[1;32m     23\u001b[0m i \u001b[39m=\u001b[39m decoder(i, initial_state\u001b[39m=\u001b[39m[j, k])\n\u001b[1;32m     24\u001b[0m i \u001b[39m=\u001b[39m Dropout(\u001b[39m.3\u001b[39m)(i)\n\u001b[0;32m---> 26\u001b[0m l \u001b[39m=\u001b[39m Attention()([x, i])\n\u001b[1;32m     27\u001b[0m ll \u001b[39m=\u001b[39m Attention()([i, x])\n\u001b[1;32m     29\u001b[0m m \u001b[39m=\u001b[39m Concatenate()([x, i, l, ll])\n",
            "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1973\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1970\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[1;32m   1971\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1972\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m-> 1973\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[1;32m   1975\u001b[0m \u001b[39m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m \u001b[39m# TF_Operation.\u001b[39;00m\n\u001b[1;32m   1977\u001b[0m \u001b[39mif\u001b[39;00m extract_traceback:\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"attention\" (type Attention).\n\nDimensions must be equal, but are 50 and 16 for '{{node attention/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=true](Placeholder, Placeholder_1)' with input shapes: [?,16,50], [?,50,16].\n\nCall arguments received by layer \"attention\" (type Attention):\n  • inputs=['tf.Tensor(shape=(None, 16, 50), dtype=float32)', 'tf.Tensor(shape=(None, 50, 16), dtype=float32)']\n  • mask=['tf.Tensor(shape=(None, 16), dtype=bool)', 'tf.Tensor(shape=(None, 50), dtype=bool)']\n  • training=None\n  • return_attention_scores=False\n  • use_causal_mask=False"
          ]
        }
      ],
      "source": [
        "\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "#               loss=masked_loss,\n",
        "#               metrics=[masked_acc])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc])\n",
        "\n",
        "\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/cap-gen/2023-05-14 08:04:36.705364-30.tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VrFvq8baLFDz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage\n",
        "initial_lr = LEARNING_RATE\n",
        "decay_rate = -0.001\n",
        "decay_steps = 2\n",
        "\n",
        "decay_callback = LearningRateDecayCallback(initial_lr, decay_rate, decay_steps)\n",
        "\n",
        "os.makedirs('log', exist_ok=True)\n",
        "csv_logger = CSVLogger('./log/training.log')\n",
        "tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0nz0_27LUSL",
        "outputId": "bf8e9211-6fa6-4ae8-bb98-9c80c64c976b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01\n"
          ]
        }
      ],
      "source": [
        "print(initial_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH--P_BE4qSV",
        "outputId": "065b9805-1d36-4554-9e46-658a4f943cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1195 66\n",
            "119 13\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "print(len(train_data) // EPOCHS, len(val_data) // EPOCHS)\n",
        "\n",
        "steps_per_epoch = int(0.1*(len(train_data) / EPOCHS))\n",
        "validation_steps =  int(.2*(len(val_data) / EPOCHS))\n",
        "print(steps_per_epoch, validation_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac3qO85h4qNE",
        "outputId": "630efa8c-3dce-46f4-9cdd-fedda1317108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "119/119 [==============================] - ETA: 0s - loss: 20.1196 - masked_acc: 4.0578e-04 - masked_loss: 20.1200"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r119/119 [==============================] - 268s 2s/step - loss: 20.1196 - masked_acc: 4.0578e-04 - masked_loss: 20.1200 - val_loss: 21.1002 - val_masked_acc: 0.0000e+00 - val_masked_loss: 21.1075\n",
            "Epoch 2/3\n",
            "119/119 [==============================] - 244s 2s/step - loss: 20.5730 - masked_acc: 8.1453e-05 - masked_loss: 20.5780 - val_loss: 21.1443 - val_masked_acc: 0.0000e+00 - val_masked_loss: 21.1778\n",
            "Epoch 3/3\n",
            "119/119 [==============================] - 212s 2s/step - loss: 20.7125 - masked_acc: 0.0000e+00 - masked_loss: 20.7103 - val_loss: 21.0854 - val_masked_acc: 0.0000e+00 - val_masked_loss: 21.1121\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_data,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=val_data,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps,\n",
        "                    callbacks=[\n",
        "                        decay_callback,\n",
        "                        csv_logger, create_model_checkpoint(model_name = 'capgen', save_dir = 'checkpoints', monitor = 'masked_acc')\n",
        "                                ]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVDBU9sFQ4bB",
        "outputId": "e6a46a4b-7a11-4ddb-8222-f51ab57fadc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqqH-Yd4qKP",
        "outputId": "c0e01f14-cab7-486c-dbec-9bcf0daf88db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# model.save(f'/content/drive/MyDrive/cap-gen/{datetime.datetime.now()}-{EPOCHS}.tf')\n",
        "\n",
        "pred = model.predict(test_data.take(1))\n",
        "# print(pred.shape)\n",
        "\n",
        "# start_token_id = word_to_id('startseq') \n",
        "# end_token_id = word_to_id('endseq') \n",
        "\n",
        "\n",
        "\n",
        "# aa = generate_caption(random_image_path, model, tokenizer)\n",
        "# print(aa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TaXI3HM4qHV",
        "outputId": "0330a2b6-417b-465c-b70e-d28d54b71d10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 50, 7653)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SEgrpR5C4qEN"
      },
      "outputs": [],
      "source": [
        "from preprocessing import tokens_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "t8Kg89PA8yWK"
      },
      "outputs": [],
      "source": [
        "\n",
        "START_TOKEN = 'startseq'\n",
        "END_TOKEN = 'endseq'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Bz1oGEdS8v7Q"
      },
      "outputs": [],
      "source": [
        "def tokens_to_text(tokens):\n",
        "    words = id_to_word(tokens)\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "    result = tf.strings.regex_replace(result, f'^ *{START_TOKEN} *', '')\n",
        "    result = tf.strings.regex_replace(result, f' *{END_TOKEN} *$', '')\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jH685-1-qhI",
        "outputId": "1ab2e204-aac3-4a01-d21b-86aba63ec608"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([8, 50])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predd = tf.argmax(pred, axis = -1)\n",
        "predd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6HzwY_t14qBZ"
      },
      "outputs": [],
      "source": [
        "rr = tokens_to_text(predd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUKOmqDW9VP-",
        "outputId": "7bb7ca06-7534-4c65-f163-4a8d88a5ec0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([8])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoATojqI9uu-",
        "outputId": "c5e75d90-04ae-4d54-8927-ef6179f515ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 2048)\n"
          ]
        }
      ],
      "source": [
        "for (img, txt_in), txt_out in test_data.take(1):\n",
        "    print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "H88_RgYW-Cba",
        "outputId": "d20c1075-4510-422f-e898-0fcc5f54043d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ce993c67-0906-462b-bb3d-3e7684b6e8d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>115745</th>\n",
              "      <td>input/flickr30k/images/4761236916.jpg</td>\n",
              "      <td>startseq A dark haired child in swimwear laugh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97590</th>\n",
              "      <td>input/flickr30k/images/4388731466.jpg</td>\n",
              "      <td>startseq Two dark haired men in orange shirts ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114665</th>\n",
              "      <td>input/flickr30k/images/4745027152.jpg</td>\n",
              "      <td>startseq A young lady is colorfully dressed in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9786</th>\n",
              "      <td>input/flickr30k/images/1557838421.jpg</td>\n",
              "      <td>startseq Two dogs wrestle with each other with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28360</th>\n",
              "      <td>input/flickr30k/images/2414352262.jpg</td>\n",
              "      <td>startseq young girl in winter coat jumping off...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce993c67-0906-462b-bb3d-3e7684b6e8d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce993c67-0906-462b-bb3d-3e7684b6e8d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce993c67-0906-462b-bb3d-3e7684b6e8d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                   image_path  \\\n",
              "115745  input/flickr30k/images/4761236916.jpg   \n",
              "97590   input/flickr30k/images/4388731466.jpg   \n",
              "114665  input/flickr30k/images/4745027152.jpg   \n",
              "9786    input/flickr30k/images/1557838421.jpg   \n",
              "28360   input/flickr30k/images/2414352262.jpg   \n",
              "\n",
              "                                                  comment  \n",
              "115745  startseq A dark haired child in swimwear laugh...  \n",
              "97590   startseq Two dark haired men in orange shirts ...  \n",
              "114665  startseq A young lady is colorfully dressed in...  \n",
              "9786    startseq Two dogs wrestle with each other with...  \n",
              "28360   startseq young girl in winter coat jumping off...  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-OtR5Ma4p9x",
        "outputId": "386c4aa1-cb98-4237-f429-91cef71c1c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
              "array([b'power power power power power power power power power power power power power power power arts arts expressing power conducts power arts power besides learn rolling arts power arts power impeach rolling rolling server trudging power releasing besides lightly rolling trudging impeach conducts wardrobe power power impeach impeach power besides',\n",
              "       b'power power power power power power power power power power power power power power power power arts expressing power conducts power arts power besides teams rolling arts power arts power impeach rolling wardrobe server trudging power couples besides lightly rolling trudging impeach rolling wardrobe power power impeach impeach power besides',\n",
              "       b'power power power power power power power power power power power power power power power arts arts expressing power conducts power arts power besides power rolling arts power arts power impeach conducts rolling server rolling power besides besides write rolling trudging impeach conducts wardrobe power power impeach impeach power besides',\n",
              "       b'power power power power power power power power power power power power power power power arts arts expressing power conducts power arts power power teams rolling arts power arts power impeach rolling lightly server conducts power power besides lightly rolling trudging impeach arts wardrobe power power conducts impeach power await',\n",
              "       b'power power power power power power power power power power power power power power power arts arts expressing power conducts power arts power besides teams rolling arts power arts power impeach rolling lightly server trudging power besides besides expressing rolling trudging impeach arts wardrobe power power conducts impeach power besides',\n",
              "       b'power power power power power power power power power power power power power power power power power expressing power conducts power arts power besides teams rolling arts power arts power impeach rolling wardrobe server trudging power couples besides lightly rolling trudging impeach conducts wardrobe power power conducts impeach power besides',\n",
              "       b'power power power power power power power power power power power power power power power arts arts expressing power conducts power arts power power sun rolling arts power arts power impeach rolling rolling server trudging power besides besides power rolling trudging impeach rolling wardrobe power power impeach impeach power arts',\n",
              "       b'power power power power power power power power power power power power power power power power power expressing power conducts power arts power besides sitar rolling arts power arts power impeach rolling lightly server trudging power besides besides lightly rolling trudging impeach arts wardrobe power power conducts impeach power besides'],\n",
              "      dtype=object)>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QX164-rr_P98"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from preprocessing import load_images_now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "liRul0Ne_Z2d"
      },
      "outputs": [],
      "source": [
        "start_token = START_TOKEN\n",
        "end_token = END_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "C_WTYK-39Z9n"
      },
      "outputs": [],
      "source": [
        "# def generate_caption(image_path, model, tokenizer, beam_size=5):\n",
        "#     ii = plt.imread(image_path)\n",
        "#     plt.imshow(ii)\n",
        "\n",
        "#     features = load_images_now(image_path)\n",
        "#     features = tf.reshape(features, (1, features.shape[0]))\n",
        "\n",
        "#     beams = [[start_token]] * beam_size\n",
        "\n",
        "#     for _ in range(MAX_LEN):\n",
        "#         for i, beam in enumerate(beams):\n",
        "#             sequence = tokenizer([beam.decode('utf-8')])\n",
        "#             sequence = tf.pad(sequence, [[0, 0], [0, MAX_LEN - tf.shape(sequence)[1]]])\n",
        "\n",
        "#             logits = model.predict([features, sequence], verbose = 0)\n",
        "#             next_word_idx = tf.argmax(logits, axis=-1)\n",
        "\n",
        "#             next_word = id_to_word(next_word_idx.numpy()[0])\n",
        "\n",
        "#             beams[i].append(next_word)\n",
        "\n",
        "#             beams.sort(key=lambda x: x[-1], reverse=True)\n",
        "\n",
        "#             beams = beams[:beam_size]\n",
        "\n",
        "#         caption = beams[0]\n",
        "#     return ' '.join(caption[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "B635Fa78A-9h"
      },
      "outputs": [],
      "source": [
        "def generate_caption(image_path):\n",
        "\n",
        "  # Load the image and resize it to 224x224 pixels.\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
        "  image = tf.image.resize(image, (224, 224))\n",
        "\n",
        "  # Get the image features from the ResNet50V2 model.\n",
        "  image_features = resnet(image)\n",
        "\n",
        "  # Get the start and end tokens.\n",
        "  start_token = tf.constant([VOCAB_SIZE - 1], dtype=tf.int32)\n",
        "  end_token = tf.constant([VOCAB_SIZE], dtype=tf.int32)\n",
        "\n",
        "  # Initialize the decoder state.\n",
        "  decoder_state = encoder.initialize_state(image_features)\n",
        "\n",
        "  # Initialize the output sequence.\n",
        "  output_sequence = []\n",
        "\n",
        "  # Generate the caption.\n",
        "  for i in range(MAX_LEN):\n",
        "\n",
        "    # Get the next word.\n",
        "    next_word_logits, decoder_state = decoder(image_features, decoder_state)\n",
        "    next_word = tf.argmax(next_word_logits, axis=1)\n",
        "\n",
        "    # If the next word is the end token, stop generating.\n",
        "    if next_word == end_token:\n",
        "      break\n",
        "\n",
        "    # Add the next word to the output sequence.\n",
        "    output_sequence.append(next_word)\n",
        "\n",
        "  # Convert the output sequence to a string.\n",
        "  output_caption = tokenizer.inverse_transform(output_sequence)\n",
        "\n",
        "  return output_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "RNDAMqYZ_No2"
      },
      "outputs": [],
      "source": [
        "random_img_path = test['image_path'].sample(1).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "YOV6kfvA_sFi",
        "outputId": "cd3e2bea-8a8e-481d-af74-df371a1cebe7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-13b9f6b7bc26>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-a1655cc68d06>\u001b[0m in \u001b[0;36mgenerate_caption\u001b[0;34m(image_path, model, tokenizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Loop to generate the caption one word at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpredicted_word_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n        rai