{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/caption-generator/blob/main/train_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "\n",
        "import os\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    os.system('git clone https://github.com/tikendraw/caption-generator.git -q')\n",
        "    os.chdir('caption-generator')\n",
        "\n",
        "\n",
        "if not os.path.exists('funcyou'):\n",
        "\tos.system('git clone https://github.com/tikendraw/funcyou -q')\n",
        "\n",
        "os.system('pip install funcyou/. -q')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, math\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import datetime\n",
        "import sys\n",
        "from functools import cache\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import regex as re\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocessing\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization, Embedding, LSTM, GRU, Bidirectional, TimeDistributed, Dense, Attention, MultiHeadAttention, Flatten, Dropout,\n",
        "    Concatenate, Activation, GlobalAveragePooling2D\n",
        "    )\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Input, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import array_to_img, img_to_array\n",
        "import string\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, TensorBoard\n",
        "from model import LearningRateDecayCallback, get_model, masked_acc, masked_loss\n",
        "import regex as re\n",
        "from preprocessing import preprocess_text, embedding_matrix_creater, mapper\n",
        "from utils import create_model_checkpoint\n",
        "\n",
        "from preprocessing import clean_words, clean_df\n",
        "from config import config\n",
        "\n",
        "from get_data import download_dataset"
      ],
      "metadata": {
        "id": "lPhVMf_b419N",
        "outputId": "9dec4dc9-52df-429b-ff85-9598f6d80fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94668760/94668760 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_value = 12321\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "\n",
        "BATCH_SIZE =    config.BATCH_SIZE\n",
        "IMG_SIZE =      config.IMG_SIZE\n",
        "CHANNELS =      config.CHANNELS\n",
        "IMG_SHAPE =     config.IMG_SHAPE\n",
        "MAX_LEN =       config.MAX_LEN\n",
        "EPOCHS =        config.EPOCHS\n",
        "LEARNING_RATE = config.LEARNING_RATE\n",
        "UNITS =         config.UNITS\n",
        "raw_caption_file =  config.raw_caption_file\n",
        "caption_file =  config.caption_file\n",
        "image_dir =     config.image_dir\n",
        "glove_path =    config.glove_path\n",
        "TEST_SIZE =     config.TEST_SIZE\n",
        "VAL_SIZE=       config.VAL_SIZE\n",
        "EMBEDDING_DIMENSION =   config.EMBEDDING_DIMENSION \n"
      ],
      "metadata": {
        "id": "KdH2ocgf5DVi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathh = Path('/content/kaggle.json')"
      ],
      "metadata": {
        "id": "A3qtMoVA_lf_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from get_data import download_dataset"
      ],
      "metadata": {
        "id": "HL-TmQzn_9Ee",
        "outputId": "b36fce59-753b-4640-c1a4-279122e74b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4ad36da8559f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mget_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'download_dataset' from 'get_data' (/content/caption-generator/get_data.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_dataset(pathh)"
      ],
      "metadata": {
        "id": "0wi7t3Bs5KMz",
        "outputId": "a1f71545-83bf-49e4-9cc9-797cc2097a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-407d0490d509>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'download_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te1Hw6gq4hIR"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(caption_file)\n",
        "\n",
        "\n",
        "#tokenizer\n",
        "tokenizer = TextVectorization(standardize=preprocess_text)\n",
        "tokenizer.adapt(df['comment'])\n",
        "\n",
        "\n",
        "word_to_id = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "id_to_word = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "\n",
        "\n",
        "\n",
        "#GLOVE embedding\n",
        "glove_api_command = 'kaggle datasets download -d watts2/glove6b50dtxt'\n",
        "glove_url = 'https://www.kaggle.com/datasets/watts2/glove6b50dtxt'\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "\n",
        "    download_kaggle_dataset(glove_api_command)\n",
        "    os.makedirs('embedding', exist_ok = True)\n",
        "    shutil.move('glove6b50dtxt.zip', 'embedding/glove.6B.50d.zip',)\n",
        "\n",
        "\n",
        "# creating embedding matrix\n",
        "word_dict = {word: i for i, word in enumerate(tokenizer.get_vocabulary())}\n",
        "\n",
        "# Creating embedding matrix\n",
        "embedding_matrix = embedding_matrix_creater(EMBEDDING_DIMENSION, word_index=word_dict)\n",
        "\n",
        "# # Saving embedding_matrix for further use\n",
        "# np.save(\"./embedding/embedding_matrix.npy\", embedding_matrix, allow_pickle=True)\n",
        "# # compressing\n",
        "# ZipFile(\"embedding_matrix.zip\", mode=\"w\").write(\n",
        "#     \"./embedding/embedding_matrix.npy\")\n",
        "\n",
        "\n",
        "# load image model\n",
        "resnet = tf.keras.applications.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=tf.keras.layers.Input(shape=IMG_SHAPE))\n",
        "\n",
        "resnet.trainable = False\n",
        "\n",
        "\n",
        "# Creating dataset\n",
        "TEST_SIZE = config.TEST_SIZE\n",
        "VAL_SIZE =  config.VAL_SIZE\n",
        "\n",
        "train, val = train_test_split(\n",
        "    df[['image_path', 'comment']],  test_size=VAL_SIZE, random_state=11)\n",
        "train, test = train_test_split(\n",
        "    train[['image_path', 'comment']],  test_size=TEST_SIZE, random_state=11)\n",
        "\n",
        "\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train.image_path, train.comment))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test.image_path, test.comment))\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val.image_path, val.comment))\n",
        "\n",
        "\n",
        "train_data = train_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_data =   test_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_data =     val_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# resnet_output_flattened_shape = 8*8*2048\n",
        "\n",
        "print(\"Number of training samples: %d\" %\n",
        "      tf.data.experimental.cardinality(train_data))\n",
        "print(\"Number of validation samples: %d\" %\n",
        "      tf.data.experimental.cardinality(val_data))\n",
        "print(\"Number of test samples: %d\" %\n",
        "      tf.data.experimental.cardinality(test_data))\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocabulary_size()\n",
        "print(\"Vocabulary size: %d\" % VOCAB_SIZE)\n",
        "\n",
        "model, img_model = get_model(embedding_matrix, VOCAB_SIZE)\n",
        "print(model.summary(), img_model.summary())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "initial_lr = LEARNING_RATE\n",
        "decay_rate = 0.01\n",
        "decay_steps = 10\n",
        "\n",
        "decay_callback = LearningRateDecayCallback(initial_lr, decay_rate, decay_steps)\n",
        "\n",
        "os.makedirs('log', exist_ok=True)\n",
        "csv_logger = CSVLogger('./log/training.log')\n",
        "tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc, masked_loss])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "print(len(train_data) // EPOCHS, len(val_data) // EPOCHS)\n",
        "\n",
        "steps_per_epoch = int(0.1*(len(train_data) / EPOCHS))\n",
        "validation_steps =  int(.2*(len(val_data) / EPOCHS))\n",
        "print(steps_per_epoch, validation_steps)\n"
      ],
      "metadata": {
        "id": "PH--P_BE4qSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(train_data,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=val_data,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps,\n",
        "                    callbacks=[\n",
        "                        decay_callback,\n",
        "                        csv_logger, create_model_checkpoint(model_name = 'capgen', save_dir = 'checkpoints', monitor = 'masked_acc')\n",
        "                                ]\n",
        "                    )\n"
      ],
      "metadata": {
        "id": "ac3qO85h4qNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save(f'/saved_model/{datetime.datetime.now()}-{EPOCHS}.h5')\n",
        "\n",
        "pred = model.evaluate(test_data)\n",
        "print((pred))\n",
        "\n",
        "# start_token_id = word_to_id('startseq') \n",
        "# end_token_id = word_to_id('endseq') \n",
        "\n",
        "\n",
        "\n",
        "# aa = generate_caption(random_image_path, model, tokenizer)\n",
        "# print(aa)\n"
      ],
      "metadata": {
        "id": "GXqqH-Yd4qKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7TaXI3HM4qHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEgrpR5C4qEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6HzwY_t14qBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-OtR5Ma4p9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf_new",
      "language": "python",
      "name": "tf_new"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}