{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    os.system('git clone https://github.com/tikendraw/caption-generator.git -q')\n",
    "    os.chdir('caption-generator')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, math\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import datetime\n",
    "import sys\n",
    "from functools import cache\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocessing\n",
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization, Embedding, LSTM, GRU, Bidirectional, TimeDistributed, Dense, Attention, MultiHeadAttention, Flatten, Dropout,\n",
    "    Concatenate, Activation, GlobalAveragePooling2D\n",
    "    )\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array\n",
    "import string\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, TensorBoard\n",
    "from model import LearningRateDecayCallback, get_model, masked_acc, masked_loss\n",
    "import regex as re\n",
    "from preprocessing import preprocess_text, embedding_matrix_creater, mapper\n",
    "from utils import create_model_checkpoint\n",
    "\n",
    "if not os.path.exists('funcyou'):\n",
    "\tos.system('git clone https://github.com/tikendraw/funcyou -q')\n",
    "os.system('pip install funcyou/. -q')\n",
    "\n",
    "from preprocessing import clean_words, clean_df\n",
    "from config import config\n",
    "\n",
    "\n",
    "\n",
    "seed_value = 12321\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "\n",
    "BATCH_SIZE =    config.BATCH_SIZE\n",
    "IMG_SIZE =      config.IMG_SIZE\n",
    "CHANNELS =      config.CHANNELS\n",
    "IMG_SHAPE =     config.IMG_SHAPE\n",
    "MAX_LEN =       config.MAX_LEN\n",
    "EPOCHS =        config.EPOCHS\n",
    "LEARNING_RATE = config.LEARNING_RATE\n",
    "UNITS =         config.UNITS\n",
    "raw_caption_file =  config.raw_caption_file\n",
    "caption_file =  config.caption_file\n",
    "image_dir =     config.image_dir\n",
    "glove_path =    config.glove_path\n",
    "TEST_SIZE =     config.TEST_SIZE\n",
    "VAL_SIZE=       config.VAL_SIZE\n",
    "EMBEDDING_DIMENSION =   config.EMBEDDING_DIMENSION \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(caption_file)\n",
    "\n",
    "\n",
    "#tokenizer\n",
    "tokenizer = TextVectorization(standardize=preprocess_text)\n",
    "tokenizer.adapt(df['comment'])\n",
    "\n",
    "\n",
    "word_to_id = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
    "id_to_word = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
    "\n",
    "\n",
    "\n",
    "#GLOVE embedding\n",
    "glove_api_command = 'kaggle datasets download -d watts2/glove6b50dtxt'\n",
    "glove_url = 'https://www.kaggle.com/datasets/watts2/glove6b50dtxt'\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    download_kaggle_dataset(glove_api_command)\n",
    "    os.makedirs('embedding', exist_ok = True)\n",
    "    shutil.move('glove6b50dtxt.zip', 'embedding/glove.6B.50d.zip',)\n",
    "\n",
    "\n",
    "# creating embedding matrix\n",
    "word_dict = {word: i for i, word in enumerate(tokenizer.get_vocabulary())}\n",
    "\n",
    "# Creating embedding matrix\n",
    "embedding_matrix = embedding_matrix_creater(EMBEDDING_DIMENSION, word_index=word_dict)\n",
    "\n",
    "# # Saving embedding_matrix for further use\n",
    "# np.save(\"./embedding/embedding_matrix.npy\", embedding_matrix, allow_pickle=True)\n",
    "# # compressing\n",
    "# ZipFile(\"embedding_matrix.zip\", mode=\"w\").write(\n",
    "#     \"./embedding/embedding_matrix.npy\")\n",
    "\n",
    "\n",
    "# load image model\n",
    "resnet = tf.keras.applications.ResNet50V2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=tf.keras.layers.Input(shape=IMG_SHAPE))\n",
    "\n",
    "resnet.trainable = False\n",
    "\n",
    "\n",
    "# Creating dataset\n",
    "TEST_SIZE = config.TEST_SIZE\n",
    "VAL_SIZE =  config.VAL_SIZE\n",
    "\n",
    "train, val = train_test_split(\n",
    "    df[['image_path', 'comment']],  test_size=VAL_SIZE, random_state=11)\n",
    "train, test = train_test_split(\n",
    "    train[['image_path', 'comment']],  test_size=TEST_SIZE, random_state=11)\n",
    "\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train.image_path, train.comment))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test.image_path, test.comment))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((val.image_path, val.comment))\n",
    "\n",
    "\n",
    "train_data = train_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_data =   test_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_data =     val_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# resnet_output_flattened_shape = 8*8*2048\n",
    "\n",
    "print(\"Number of training samples: %d\" %\n",
    "      tf.data.experimental.cardinality(train_data))\n",
    "print(\"Number of validation samples: %d\" %\n",
    "      tf.data.experimental.cardinality(val_data))\n",
    "print(\"Number of test samples: %d\" %\n",
    "      tf.data.experimental.cardinality(test_data))\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocabulary_size()\n",
    "print(\"Vocabulary size: %d\" % VOCAB_SIZE)\n",
    "\n",
    "model, img_model = get_model(embedding_matrix, VOCAB_SIZE)\n",
    "print(model.summary(), img_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "initial_lr = LEARNING_RATE\n",
    "decay_rate = 0.01\n",
    "decay_steps = 10\n",
    "\n",
    "decay_callback = LearningRateDecayCallback(initial_lr, decay_rate, decay_steps)\n",
    "\n",
    "os.makedirs('log', exist_ok=True)\n",
    "csv_logger = CSVLogger('./log/training.log')\n",
    "tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "              loss=masked_loss,\n",
    "              metrics=[masked_acc, masked_loss])\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "print(len(train_data) // EPOCHS, len(val_data) // EPOCHS)\n",
    "\n",
    "steps_per_epoch = int(0.1*(len(train_data) / EPOCHS))\n",
    "validation_steps =  int(.2*(len(val_data) / EPOCHS))\n",
    "print(steps_per_epoch, validation_steps)\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=val_data,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[\n",
    "                        decay_callback,\n",
    "                        csv_logger, create_model_checkpoint(model_name = 'capgen', save_dir = 'checkpoints', monitor = 'masked_acc')\n",
    "                                ]\n",
    "                    )\n",
    "\n",
    "model.save(f'/saved_model/{datetime.datetime.now()}-{EPOCHS}.h5')\n",
    "\n",
    "pred = model.evaluate(test_data)\n",
    "print((pred))\n",
    "\n",
    "# start_token_id = word_to_id('startseq') \n",
    "# end_token_id = word_to_id('endseq') \n",
    "\n",
    "\n",
    "\n",
    "# aa = generate_caption(random_image_path, model, tokenizer)\n",
    "# print(aa)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "tf_new"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
