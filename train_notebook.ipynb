{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/caption-generator/blob/main/train_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPhVMf_b419N",
        "outputId": "e183fd0b-c15c-4f55-fb8c-efa0669384ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-15 11:12:43.150388: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-15 11:12:44.040612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-15 11:12:45.798495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:45.855152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:45.855347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:45.856246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:45.856407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:45.856546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:46.639663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:46.639923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:46.640082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-15 11:12:46.640233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2018 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
            "[nltk_data] Downloading package punkt to /home/t/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    os.system('git clone https://github.com/tikendraw/caption-generator.git -q')\n",
        "    os.chdir('caption-generator')\n",
        "\n",
        "\n",
        "if not os.path.exists('funcyou'):\n",
        "\tos.system('git clone https://github.com/tikendraw/funcyou -q')\n",
        "\n",
        "# os.system('pip install funcyou/. -q')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, math\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import datetime\n",
        "import sys\n",
        "from functools import cache\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import regex as re\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocessing\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization, Embedding, LSTM, GRU, Bidirectional, TimeDistributed, Dense, Attention, MultiHeadAttention, Flatten, Dropout,\n",
        "    Concatenate, Activation, GlobalAveragePooling2D\n",
        "    )\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Input, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import array_to_img, img_to_array\n",
        "import string\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, TensorBoard\n",
        "from model import LearningRateDecayCallback, get_model, masked_acc, masked_loss\n",
        "from preprocessing import preprocess_text, embedding_matrix_creater, mapper, clean_words, clean_df\n",
        "from utils import create_model_checkpoint\n",
        "\n",
        "from config import config\n",
        "\n",
        "from get_data import download_dataset\n",
        "from funcyou.dataset import download_kaggle_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdH2ocgf5DVi"
      },
      "outputs": [],
      "source": [
        "seed_value = 12321\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "\n",
        "BATCH_SIZE =    config.BATCH_SIZE\n",
        "IMG_SIZE =      config.IMG_SIZE\n",
        "CHANNELS =      config.CHANNELS\n",
        "IMG_SHAPE =     config.IMG_SHAPE\n",
        "MAX_LEN =       config.MAX_LEN\n",
        "EPOCHS =        config.EPOCHS\n",
        "LEARNING_RATE = config.LEARNING_RATE\n",
        "UNITS =         config.UNITS\n",
        "raw_caption_file =  config.raw_caption_file\n",
        "caption_file =  config.caption_file\n",
        "image_dir =     config.image_dir\n",
        "glove_path =    config.glove_path\n",
        "TEST_SIZE =     config.TEST_SIZE\n",
        "VAL_SIZE=       config.VAL_SIZE\n",
        "EMBEDDING_DIMENSION =   config.EMBEDDING_DIMENSION "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3qtMoVA_lf_"
      },
      "outputs": [],
      "source": [
        "pathh = '/content/kaggle.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wi7t3Bs5KMz",
        "outputId": "fa0906b5-4aff-47ab-9c18-cf54877fac74"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "    download_dataset(pathh)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S2RsNzwVEoIU"
      },
      "source": [
        "# Reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "IxFlSCnqDIxq",
        "outputId": "c2e4220b-b5d1-4bdf-df21-52ec328d9a51"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(caption_file)\n",
        "\n",
        "print(df.info())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVORBQxSE2tn",
        "outputId": "fda52343-0b53-4914-97ae-18566948c303"
      },
      "outputs": [],
      "source": [
        "df = df[df.comment_number == 1]\n",
        "print(df.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aTMyFjg3C4mo"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg2RRXBWC3pQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#tokenizer\n",
        "tokenizer = TextVectorization(standardize=preprocess_text)\n",
        "tokenizer.adapt(df['comment'])\n",
        "\n",
        "\n",
        "word_to_id = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "id_to_word = tf.keras.layers.StringLookup(vocabulary=tokenizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1Hw6gq4hIR",
        "outputId": "7473236f-345e-48f5-a9e3-d17489886c5f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#GLOVE embedding\n",
        "glove_api_command = 'kaggle datasets download -d watts2/glove6b50dtxt'\n",
        "glove_url = 'https://www.kaggle.com/datasets/watts2/glove6b50dtxt'\n",
        "\n",
        "# if 'google.colab' in sys.modules:\n",
        "\n",
        "#     download_kaggle_dataset(glove_api_command)\n",
        "#     os.makedirs('embedding', exist_ok = True)\n",
        "#     shutil.move('glove6b50dtxt.zip', 'embedding/glove.6B.50d.zip',)\n",
        "\n",
        "\n",
        "# creating embedding matrix\n",
        "word_dict = {word: i for i, word in enumerate(tokenizer.get_vocabulary())}\n",
        "\n",
        "# Creating embedding matrix\n",
        "embedding_matrix = embedding_matrix_creater(EMBEDDING_DIMENSION, word_index=word_dict)\n",
        "\n",
        "# # Saving embedding_matrix for further use\n",
        "# np.save(\"./embedding/embedding_matrix.npy\", embedding_matrix, allow_pickle=True)\n",
        "# # compressing\n",
        "# ZipFile(\"embedding_matrix.zip\", mode=\"w\").write(\n",
        "#     \"./embedding/embedding_matrix.npy\")\n",
        "\n",
        "\n",
        "# load image model\n",
        "resnet = tf.keras.applications.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=tf.keras.layers.Input(shape=IMG_SHAPE))\n",
        "\n",
        "resnet.trainable = False\n",
        "\n",
        "\n",
        "# Creating dataset\n",
        "TEST_SIZE = config.TEST_SIZE\n",
        "VAL_SIZE =  config.VAL_SIZE\n",
        "\n",
        "train, val = train_test_split(\n",
        "    df[['image_path', 'comment']],  test_size=VAL_SIZE, random_state=11)\n",
        "train, test = train_test_split(\n",
        "    train[['image_path', 'comment']],  test_size=TEST_SIZE, random_state=11)\n",
        "\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train.image_path, train.comment))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test.image_path, test.comment))\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val.image_path, val.comment))\n",
        "\n",
        "\n",
        "train_data = train_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_data =   test_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_data =     val_data.map(lambda x,y:mapper(x, y, tokenizer)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# resnet_output_flattened_shape = 8*8*2048\n",
        "\n",
        "print(\"Number of training samples: %d\" %\n",
        "      tf.data.experimental.cardinality(train_data))\n",
        "print(\"Number of validation samples: %d\" %\n",
        "      tf.data.experimental.cardinality(val_data))\n",
        "print(\"Number of test samples: %d\" %\n",
        "      tf.data.experimental.cardinality(test_data))\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocabulary_size()\n",
        "print(\"Vocabulary size: %d\" % VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSSztQsfRuzJ"
      },
      "outputs": [],
      "source": [
        "model = get_model(embedding_matrix, VOCAB_SIZE)\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc, masked_loss])\n",
        "\n",
        "\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/cap-gen/2023-05-14 08:04:36.705364-30.tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrFvq8baLFDz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage\n",
        "initial_lr = LEARNING_RATE\n",
        "decay_rate = 0.001\n",
        "decay_steps = 10\n",
        "\n",
        "decay_callback = LearningRateDecayCallback(initial_lr, decay_rate, decay_steps)\n",
        "\n",
        "os.makedirs('log', exist_ok=True)\n",
        "csv_logger = CSVLogger('./log/training.log')\n",
        "tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0nz0_27LUSL",
        "outputId": "eb53c04c-e57f-4555-c62a-babab7ebb6a0"
      },
      "outputs": [],
      "source": [
        "print(initial_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH--P_BE4qSV",
        "outputId": "679f95af-be36-427a-df35-059d0b840be4"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "print(len(train_data) // EPOCHS, len(val_data) // EPOCHS)\n",
        "\n",
        "steps_per_epoch = int(0.1*(len(train_data) / EPOCHS))\n",
        "validation_steps =  int(.2*(len(val_data) / EPOCHS))\n",
        "print(steps_per_epoch, validation_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac3qO85h4qNE",
        "outputId": "f2fc1299-2967-42bc-da15-ab5220fd5e10"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_data,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=val_data,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps,\n",
        "                    callbacks=[\n",
        "                        # decay_callback,\n",
        "                        csv_logger, create_model_checkpoint(model_name = 'capgen', save_dir = 'checkpoints', monitor = 'masked_acc')\n",
        "                                ]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lr0gYQwPNAK"
      },
      "outputs": [],
      "source": [
        "from funcyou.plot import plot_history\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "h9gzGrQnPVVg",
        "outputId": "3e870997-74f8-4266-f9d3-ce1cab0bb99f"
      },
      "outputs": [],
      "source": [
        "plot_history(history, plot = ['masked_loss','masked_acc'], split = ['train','val'], epoch= EPOCHS, figsize = (20,10), colors = ['r','b'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVDBU9sFQ4bB",
        "outputId": "8d7c4dc5-f506-4702-a06b-0766288b4e9c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqqH-Yd4qKP",
        "outputId": "dbed18b1-ce37-4652-8ff8-3d1cda974be5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model.save(f'/content/drive/MyDrive/cap-gen/{datetime.datetime.now()}-{EPOCHS}.tf')\n",
        "\n",
        "pred = model.predict(test_data.take(1))\n",
        "# print((pred.shape))\n",
        "\n",
        "# start_token_id = word_to_id('startseq') \n",
        "# end_token_id = word_to_id('endseq') \n",
        "\n",
        "\n",
        "\n",
        "# aa = generate_caption(random_image_path, model, tokenizer)\n",
        "# print(aa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TaXI3HM4qHV",
        "outputId": "bb9eb9e1-1aee-4359-c436-ab38ab8cb437"
      },
      "outputs": [],
      "source": [
        "pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEgrpR5C4qEN"
      },
      "outputs": [],
      "source": [
        "from preprocessing import tokens_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "6HzwY_t14qBZ",
        "outputId": "deb81666-ae72-4843-b8d5-3fff2557b228"
      },
      "outputs": [],
      "source": [
        "tokens_to_text(pred, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-OtR5Ma4p9x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_new",
      "language": "python",
      "name": "tf_new"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
